{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOUX4dv+gqSUel4/9l5n2lb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vaibhav074N/Assignment-Mar17/blob/main/Assignment_Mar17.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1: What are missing values in a dataset? Why is it essential to handle missing values? Name some\n",
        "algorithms that are not affected by missing values."
      ],
      "metadata": {
        "id": "rge4c-mwpj7v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Missing values in a dataset are values that are not available for some variables or observations. Missing values can occur for several reasons, such as data entry errors, incomplete data, or missing responses in a survey.\n",
        "\n",
        "It is essential to handle missing values in a dataset because they can lead to biased or inaccurate analysis results. When missing values are not handled properly, it can lead to a loss of statistical power, reduced sample size, and incorrect conclusions. Handling missing values is an important step in data preprocessing and can improve the accuracy and reliability of the analysis.\n",
        "\n",
        "Some algorithms that are not affected by missing values include decision trees, random forests, and gradient boosting machines. These algorithms can handle missing values by treating them as a separate category or by imputing missing values with a default value. Other algorithms, such as linear regression and logistic regression, require imputation of missing values or deletion of the observations with missing values before analysis. It is important to check the assumptions of each algorithm before choosing an appropriate method for handling missing values."
      ],
      "metadata": {
        "id": "NqBFWj8MqhiS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFmEjmI8pc8A"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2: List down techniques used to handle missing data. Give an example of each with python code."
      ],
      "metadata": {
        "id": "JyBQhMYeqkEv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.Deletion: This technique involves deleting the observations or variables with missing values.\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({'A': [1, 2, None, 4], 'B': [5, None, 7, 8]})\n",
        "\n",
        "df_dropped = df.dropna()\n",
        "\n",
        "print(df_dropped)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fy-pRpbJqpNc",
        "outputId": "cbecc5f9-5ebd-4644-8a18-6821d1c5126e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     A    B\n",
            "0  1.0  5.0\n",
            "3  4.0  8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mean/Mode/Median Imputation:-This technique involves replacing missing values with the mean, mode, or median of the available values in the variable.\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "df = pd.DataFrame({'A': [1, 2, 3, None, 5], 'B': [6, None, 8, None, 10]})\n",
        "\n",
        "# Replace missing values with mean\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
        "\n",
        "print(df_imputed)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANWsn78rrZB9",
        "outputId": "9db57f2b-f791-4ba0-d0dc-0a52a0925805"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      A     B\n",
            "0  1.00   6.0\n",
            "1  2.00   8.0\n",
            "2  3.00   8.0\n",
            "3  2.75   8.0\n",
            "4  5.00  10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.Interpolation:- In this technique, the missing values are estimated based on the values of adjacent data points. This technique is useful when the missing values are evenly distributed across the dataset.\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({'A': [1, 2, 3, None, 5], 'B': [6, None, 8, None, 10]})\n",
        "\n",
        "# Interpolate missing values\n",
        "df_interpolated = df.interpolate()\n",
        "\n",
        "print(df_interpolated)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsrEb-hvs7Th",
        "outputId": "0d786469-5790-41a0-a4a3-8d60e4c9ec99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     A     B\n",
            "0  1.0   6.0\n",
            "1  2.0   7.0\n",
            "2  3.0   8.0\n",
            "3  4.0   9.0\n",
            "4  5.0  10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.Forward and Backward Fill:- In this technique, the missing values are replaced with the previous or next value in the dataset\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "data = {'A': [1, 2, None, 4, 5], 'B': [None, 6, 7, None, 9]}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# forward fill missing values\n",
        "df_ffill = df.fillna(method='ffill')\n",
        "\n",
        "# backward fill missing values\n",
        "df_bfill = df.fillna(method='bfill')\n",
        "\n",
        "print(df)\n",
        "print(df_ffill)\n",
        "print(df_bfill)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIxNkLsLtMWA",
        "outputId": "8546ca08-57f0-477a-d4aa-296fd2934749"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     A    B\n",
            "0  1.0  NaN\n",
            "1  2.0  6.0\n",
            "2  NaN  7.0\n",
            "3  4.0  NaN\n",
            "4  5.0  9.0\n",
            "     A    B\n",
            "0  1.0  NaN\n",
            "1  2.0  6.0\n",
            "2  2.0  7.0\n",
            "3  4.0  7.0\n",
            "4  5.0  9.0\n",
            "     A    B\n",
            "0  1.0  6.0\n",
            "1  2.0  6.0\n",
            "2  4.0  7.0\n",
            "3  4.0  9.0\n",
            "4  5.0  9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HaBh9yICtzuJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3: Explain the imbalanced data. What will happen if imbalanced data is not handled?"
      ],
      "metadata": {
        "id": "Frzgu6X0t-7b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If imbalanced data is not handled properly, it can lead to biased model performance and incorrect predictions. The model trained on imbalanced data tends to be more accurate in predicting the majority class but performs poorly in predicting the minority class. This is because the model is more likely to classify all instances as the majority class to optimize its accuracy, resulting in poor predictive performance for the minority class.\n",
        "\n",
        "For example, suppose we have a dataset of 1000 credit card transactions, where only 10 of them are fraudulent transactions. If we train a model on this imbalanced dataset without balancing the classes, the model will likely predict all transactions as non-fraudulent to optimize its accuracy, resulting in poor predictive performance for the minority class.\n",
        "\n",
        "To avoid such situations, several techniques can be used to handle imbalanced data, including oversampling the minority class, undersampling the majority class."
      ],
      "metadata": {
        "id": "f-5qjsSausrn"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M-hOCnAOt_at"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4: What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down-\n",
        "sampling are required."
      ],
      "metadata": {
        "id": "P8RmT1dPu88U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Up-sampling is a technique that increases the number of samples in the minority class to balance the class distribution. This can be done by randomly duplicating existing samples in the minority class or by generating new synthetic samples using techniques such as the Synthetic Minority Over-sampling Technique (SMOTE).\n",
        "\n",
        "Example:\n",
        "Suppose we have a dataset of 1000 credit card transactions, where only 10 of them are fraudulent transactions. In this case, we can use up-sampling to increase the number of fraudulent transactions by generating synthetic samples. For instance, we can use SMOTE to generate new fraudulent transactions by randomly selecting a fraudulent transaction and creating a new transaction with a combination of its features and those of its nearest neighbors.\n",
        "\n",
        "Down-sampling, on the other hand, is a technique that reduces the number of samples in the majority class to balance the class distribution. This can be done by randomly removing samples from the majority class or by selecting a subset of the majority class that is equal in size to the minority class.\n",
        "\n",
        "Example:\n",
        "Suppose we have a dataset of 1000 credit card transactions, where only 10 of them are fraudulent transactions. In this case, we can use down-sampling to reduce the number of non-fraudulent transactions by randomly selecting a subset of transactions that is equal in size to the fraudulent transactions.\n",
        "\n",
        "The decision to use up-sampling or down-sampling depends on the specific problem and the available data. If the minority class has enough representative samples and the dataset is not too large, up-sampling can be a good choice. However, if the dataset is very large or the minority class has very few samples, down-sampling can be a better choice. In some cases, a combination of up-sampling and down-sampling can also be used to balance the class distribution"
      ],
      "metadata": {
        "id": "KeN_k9_AwuDI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N_zUWlTOwfur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5: What is data Augmentation? Explain SMOTE."
      ],
      "metadata": {
        "id": "dKFDOvpOxDkW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data augmentation is a technique used to increase the size of a dataset by creating new samples from the existing data. The idea is to introduce variations in the input data that can help the model generalize better and avoid overfitting. Data augmentation can be applied in various ways, such as flipping, rotating, cropping, zooming, or adding noise to images or text data."
      ],
      "metadata": {
        "id": "0d_-aS831tyf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SMOTE (Synthetic Minority Over-sampling Technique) is a popular data augmentation technique used to handle imbalanced data. The idea behind SMOTE is to create new synthetic samples from the minority class by interpolating between existing samples.\n",
        "\n",
        "The basic steps of the SMOTE algorithm are as follows:\n",
        "\n",
        "1.Select a minority class sample at random.\n",
        "\n",
        "2.Find the k nearest neighbors of the sample in the feature space.\n",
        "\n",
        "3.Select one of the k neighbors at random.\n",
        "\n",
        "4.Generate a new synthetic sample by interpolating between the selected sample and the neighbor.\n",
        "\n",
        "The interpolation is done by selecting a random point along the line segment that connects the two samples in the feature space. This produces a new sample that lies on the line between the two samples and adds new information to the dataset."
      ],
      "metadata": {
        "id": "DcVXDr5d2MyV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cS1kh6obxg1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6: What are outliers in a dataset? Why is it essential to handle outliers?"
      ],
      "metadata": {
        "id": "FNHuIDeW2dl4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Outliers are data points that are significantly different from other data points in a dataset. They can be caused by measurement errors, data entry errors, or natural variation in the data. Outliers can have a significant impact on statistical analysis and machine learning models."
      ],
      "metadata": {
        "id": "Q8g8gE5j2yer"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is essential to handle outliers for several reasons:\n",
        "\n",
        "- They can skew the distribution of the data, making it non-normal and affecting the results of statistical analysis that assume a normal distribution.\n",
        "\n",
        "- They can affect the measures of central tendency, such as the mean and median, leading to biased estimates.\n",
        "\n",
        "- They can affect the measures of variability, such as the standard deviation and variance, leading to incorrect estimates of the spread of the data.\n",
        "\n",
        "- They can affect the performance of machine learning models, leading to overfitting or underfitting."
      ],
      "metadata": {
        "id": "f4gTrH0W2_aY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Handling outliers involves identifying them and deciding how to deal with them. This can be done using various techniques, such as:\n",
        "\n",
        "- Visual inspection: plotting the data and looking for data points that are significantly different from other points.\n",
        "\n",
        "- Statistical methods: using statistical tests to identify outliers based on the distribution of the data.\n",
        "\n",
        "- Z-score method: calculating the z-score of each data point and removing data points with z-scores above a certain threshold.\n",
        "\n",
        "- Winsorizing: replacing the outliers with the nearest non-outlier values."
      ],
      "metadata": {
        "id": "8_q4Cv0U3cV6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In summary, handling outliers is essential to ensure the accuracy and reliability of statistical analysis and machine learning models. It involves identifying and dealing with data points that are significantly different from other points in the dataset using various techniques."
      ],
      "metadata": {
        "id": "ZheXGtGS6gcx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "763GZ-3q2eKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7: You are working on a project that requires analyzing customer data. However, you notice that some of\n",
        "the data is missing. What are some techniques you can use to handle the missing data in your analysis?"
      ],
      "metadata": {
        "id": "daHhwK5p69PQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are several techniques that can be used to handle missing data in customer data analysis:\n",
        "\n",
        "- Complete Case Analysis (CCA): This technique involves discarding any data point with missing values, assuming that the remaining data is sufficient to produce reliable results. This technique is straightforward but can result in a significant loss of data and reduce the representativeness of the sample.\n",
        "\n",
        "- Mean/Median/Mode Imputation: This technique involves replacing the missing values with the mean, median or mode value of the corresponding feature. This technique is simple and can be effective if the amount of missing data is small and the distribution of the data is not significantly affected by outliers.\n",
        "\n",
        "- Regression Imputation: This technique involves using a regression model to predict the missing values based on the values of other features. This technique can be effective if the relationship between the missing value and the other features is strong and the model used for prediction is accurate.\n",
        "\n",
        "- Multiple Imputation: This technique involves creating multiple imputed datasets using statistical methods and combining the results to produce a final estimate. This technique can be effective if the amount of missing data is large and the missing data is missing at random"
      ],
      "metadata": {
        "id": "DJarW8Tk7-ZG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fRPAWNTT6-JF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8: You are working with a large dataset and find that a small percentage of the data is missing. What are\n",
        "some strategies you can use to determine if the missing data is missing at random or if there is a pattern\n",
        "to the missing data?"
      ],
      "metadata": {
        "id": "FQ0AZZPG9Q42"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When working with a large dataset, it is important to determine whether the missing data is missing at random (MAR) or if there is a pattern to the missing data, such as missing completely at random (MCAR) or missing not at random (MNAR). \n",
        "\n",
        "Here are some strategies to determine the nature of the missing data:\n",
        "\n",
        "1.Visualization: Visualizing the missing data using graphs or plots can help identify any patterns in the missing data. For example, a heat map or correlation plot can help identify if there is a relationship between the missing data and other variables in the dataset.\n",
        "\n",
        "2.Statistical tests: Statistical tests can be used to determine if the missing data is MAR or not. For example, the Little's MCAR test can be used to test whether the missing data is MCAR, while multiple imputation can be used to test whether the missing data is MAR or MNAR.\n",
        "\n",
        "3.Domain knowledge: Expert knowledge in the domain of the data can help identify any patterns in the missing data. For example, if the missing data is related to demographic variables, it may be due to the respondents choosing not to answer the question.\n",
        "\n",
        "4.Pattern recognition algorithms: Machine learning algorithms such as decision trees, random forests, and logistic regression can be used to identify patterns in the missing data. These algorithms can help determine which variables are related to the missing data and how the missing data is distributed across the dataset.\n",
        "\n",
        "In summary, determining the nature of the missing data is crucial for choosing an appropriate strategy for handling the missing data. A combination of visualization, statistical tests, domain knowledge, and pattern recognition algorithms can help determine if the missing data is missing at random or if there is a pattern to the missing data."
      ],
      "metadata": {
        "id": "Uc8W2yxY9rGD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vhLOBX8u9RYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9: Suppose you are working on a medical diagnosis project and find that the majority of patients in the\n",
        "dataset do not have the condition of interest, while a small percentage do. What are some strategies you\n",
        "can use to evaluate the performance of your machine learning model on this imbalanced dataset?"
      ],
      "metadata": {
        "id": "xbEMK95w95d2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When working with an imbalanced medical diagnosis dataset, where the majority of patients do not have the condition of interest, while a small percentage do, it is essential to use appropriate strategies to evaluate the performance of the machine learning model. Here are some strategies that can be used:\n",
        "\n",
        "\n",
        "- Confusion Matrix: A confusion matrix is a table that summarizes the performance of a machine learning model. It shows the number of true positives, false positives, true negatives, and false negatives. This matrix can be used to calculate metrics such as accuracy, precision, recall, and F1 score, which are important indicators of the performance of the model on an imbalanced dataset.\n",
        "\n",
        "- Stratified Sampling: Stratified sampling involves dividing the dataset into strata based on the outcome variable and then randomly sampling from each stratum to ensure that the sample is representative of the population. This can be particularly useful in imbalanced datasets, where the minority class may be underrepresented in the sample.\n",
        "\n",
        "- Cross-Validation: Cross-validation involves dividing the dataset into k subsets and using each subset as a test set while the remaining subsets are used as training data. This technique can be particularly useful in imbalanced datasets, as it allows for a more reliable estimate of the model's performance on new data.\n",
        "\n",
        "- Resampling Techniques: Resampling techniques such as oversampling and undersampling can be used to balance the dataset. Oversampling involves replicating the minority class samples, while undersampling involves randomly removing samples from the majority class. These techniques can help balance the dataset and improve the performance of the model.\n",
        "\n",
        "- Evaluation Metrics: It is important to use evaluation metrics that are appropriate for imbalanced datasets. Metrics such as precision, recall, and F1 score are particularly useful for evaluating the performance of a model on imbalanced datasets, as they take into account the class imbalance.\n",
        "\n",
        "In summary, when working with an imbalanced medical diagnosis dataset, it is essential to use appropriate strategies to evaluate the performance of the machine learning model. The strategies include using a confusion matrix, stratified sampling, cross-validation, resampling techniques, and evaluation metrics. These strategies can help ensure that the model's performance is reliable and can be used in real-world scenarios."
      ],
      "metadata": {
        "id": "Zrikxh4CQThD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pm6iyIt19583"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q10: When attempting to estimate customer satisfaction for a project, you discover that the dataset is\n",
        "unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to\n",
        "balance the dataset and down-sample the majority class?"
      ],
      "metadata": {
        "id": "Zc6jSuzr98S1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When dealing with an unbalanced dataset in customer satisfaction estimation, where the majority of customers report being satisfied, we can use various techniques to balance the dataset and down-sample the majority class. Here are some methods that can be employed:\n",
        "\n",
        "- Random Under-sampling: This method involves randomly removing samples from the majority class until the dataset is balanced. However, this method may result in the loss of useful information, especially if the dataset is already small.\n",
        "\n",
        "- Synthetic Minority Over-sampling Technique (SMOTE): This method involves creating synthetic minority class samples by interpolating between existing minority class samples. This method can be effective in generating new samples without losing any information.\n",
        "\n",
        "- Tomek Links: This method involves identifying samples from the majority class that are closest to samples from the minority class and removing them. This can help in removing noise from the majority class.\n",
        "\n",
        "- Edited Nearest Neighbors (ENN): This method involves removing samples from the majority class that are misclassified by the nearest neighbors classifier. This can help in removing noisy samples from the majority class.\n",
        "\n",
        "- Cluster-Based Over-sampling: This method involves identifying clusters of minority class samples and creating synthetic samples within each cluster. This can help in creating diverse synthetic samples and improving the performance of the model.\n",
        "\n",
        "In summary, when dealing with an unbalanced dataset in customer satisfaction estimation, we can use various techniques to balance the dataset and down-sample the majority class. The methods include random under-sampling, SMOTE, Tomek Links, ENN, and cluster-based over-sampling. These techniques can help in improving the performance of the model by addressing the class imbalance problem."
      ],
      "metadata": {
        "id": "Jns1nzeVQ0kK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MNzXMlWw982l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q11: You discover that the dataset is unbalanced with a low percentage of occurrences while working on a\n",
        "project that requires you to estimate the occurrence of a rare event. What methods can you employ to\n",
        "balance the dataset and up-sample the minority class?"
      ],
      "metadata": {
        "id": "byns6i_b9_o4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When dealing with a dataset that is unbalanced with a low percentage of occurrences, here are some methods that can be employed to balance the dataset and up-sample the minority class:\n",
        "\n",
        "- Random Over-sampling: This method involves randomly duplicating samples from the minority class until the dataset is balanced. However, this method may result in overfitting and reduce the model's performance on unseen data.\n",
        "\n",
        "- Synthetic Minority Over-sampling Technique (SMOTE): This method involves creating synthetic minority class samples by interpolating between existing minority class samples. This method can be effective in generating new samples without losing any information.\n",
        "\n",
        "- Adaptive Synthetic Sampling (ADASYN): This method involves creating synthetic minority class samples using a density distribution function to generate more synthetic samples for the minority class samples that are harder to learn.\n",
        "\n",
        "- SMOTE with Tomek Links: This method combines SMOTE with Tomek links to remove noisy samples from the minority class and create synthetic minority class samples.\n",
        "\n",
        "- SMOTE with Edited Nearest Neighbors (SMOTE-ENN): This method combines SMOTE with ENN to remove noisy samples from both the majority and minority classes and create synthetic minority class samples.\n",
        "\n",
        "In summary, when dealing with an unbalanced dataset with a low percentage of occurrences, we can use various techniques to balance the dataset and up-sample the minority class. The methods include random over-sampling, SMOTE, ADASYN, SMOTE with Tomek links, and SMOTE with edited nearest neighbors. These techniques can help in improving the performance of the model by addressing the class imbalance problem. However, it is important to carefully evaluate the performance of the model on the test set to avoid overfitting."
      ],
      "metadata": {
        "id": "u30A9uVtRXP2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Tg0IOT2A-ANT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}